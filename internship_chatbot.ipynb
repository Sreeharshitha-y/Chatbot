{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing useful libraries"
      ],
      "metadata": {
        "id": "R1GUWCVXpKOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n",
        "!apt-get install tesseract-ocr\n",
        "!apt-get install libtesseract-dev\n",
        "!pip install pytesseract\n",
        "!pip install pdf2image"
      ],
      "metadata": {
        "id": "-TrVRTfAOcig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26087187-63b4-44e8-a1a9-b8f0480b4a78"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.3 [186 kB]\n",
            "Fetched 186 kB in 0s (508 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121658 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (7,794 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121688 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 3,743 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,743 kB in 0s (7,928 kB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 121735 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7UiVFbIUqCP",
        "outputId": "90aeae39-811e-449b-dc52-bc905430cbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import pytesseract\n",
        "import pdf2image\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set the path to Poppler binaries\n",
        "poppler_path = \"/usr/bin\"  # Use the default path in Colab\n",
        "\n",
        "# Optionally, set the environment variable\n",
        "os.environ[\"PYPDF2_BIN_PATH\"] = poppler_path\n",
        "os.environ[\"PATH\"] += os.pathsep + poppler_path\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "\n",
        "# Rest of your code...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to read PDF files that returns text corpus."
      ],
      "metadata": {
        "id": "U1lzwElSbCsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read PDF files that returns text corpus.\n",
        "def get_pdf_data(user_resp):\n",
        "    PDF_file = \"pdf1.pdf\"\n",
        "    pages = convert_from_path(PDF_file, 500, poppler_path=poppler_path)\n",
        "    image_counter = 1\n",
        "    for page in pages:\n",
        "        filename = \"page_\" + str(image_counter) + \".jpg\"\n",
        "        page.save(filename, 'JPEG')\n",
        "        image_counter = image_counter + 1\n",
        "\n",
        "    filelimit = image_counter - 1\n",
        "    corpus = ''\n",
        "    for i in range(1, filelimit + 1):\n",
        "        filename = \"page_\" + str(i) + \".jpg\"\n",
        "        text = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
        "        text = text.replace('-\\n', '')\n",
        "        corpus += text\n",
        "\n",
        "    sent_tokens = nltk.sent_tokenize(corpus)\n",
        "    return sent_tokens\n"
      ],
      "metadata": {
        "id": "KZAk_NArnMYd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Cleaning the data and making it usable."
      ],
      "metadata": {
        "id": "IP6gyyZCnzEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the data and making it usable.\n",
        "def LemNormalize(corpus):\n",
        "    return nltk.word_tokenize(corpus.lower().translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "# Greeting Inputs\n",
        "GREETING_INPUTS = [\"hi\", \"hello\", \"hola\", \"greetings\", \"wassup\", \"hey\"]\n",
        "# Greeting responses back to the user\n",
        "GREETING_RESPONSES=[\"howdy\", \"hi\", \"hey\", \"what's good\", \"hello\", \"hey there\"]\n",
        "# Function to return a random greeting response to a user's greeting\n",
        "def greeting(sentence):\n",
        "    # If the user's input is a greeting, then return a randomly chosen greeting response\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "metadata": {
        "id": "b7a5xxOwn14E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a text similarity detection function that matches the user inputs and returns similar sentences."
      ],
      "metadata": {
        "id": "xpb02pFWoLaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here I have used TF-IDF vectors and cosine similarity scores for matching the data with user-input\n"
      ],
      "metadata": {
        "id": "Veyw1g_moVqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a text similarity detection function that matches the user inputs and returns similar sentences.\n",
        "def response(user_response):\n",
        "    # The user's response / query\n",
        "    user_response = user_response.lower()  # Make the response lowercase\n",
        "    # Set the chatbot response to an empty string\n",
        "    robo_response = ''\n",
        "    sent_tokens = get_pdf_data(user_response)\n",
        "    # Append the user's response to the sentence list\n",
        "    sent_tokens.append(user_response)\n",
        "    # Create a TfidfVectorizer Object\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    # Convert the text to a matrix of TF-IDF features\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    # Get the measure of similarity (similarity scores)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    # Get the index of the most similar text/sentence to the user's response\n",
        "    try:\n",
        "        idx = vals.argsort()[0][-2]\n",
        "    except IndexError:\n",
        "        robo_response = robo_response + \"I apologize, I don't understand.\"\n",
        "        return robo_response\n",
        "    # Reduce the dimensionality of vals\n",
        "    flat = vals.flatten()\n",
        "    # Sort the list in ascending order\n",
        "    flat.sort()\n",
        "    # Get the most similar score to the user's response\n",
        "    score = flat[-2]\n",
        "    # If the variable 'score' is 0 then there is no text similar to the user's response\n",
        "    if score == 0:\n",
        "        robo_response = robo_response + \"I apologize, I don't understand.\"\n",
        "    else:\n",
        "        robo_response = robo_response + sent_tokens[idx]\n",
        "\n",
        "    # Remove the user's response from the sentence tokens list\n",
        "    sent_tokens.remove(user_response)\n",
        "\n",
        "    return robo_response"
      ],
      "metadata": {
        "id": "IfYu1SPboaYr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A function that handles the input of the users."
      ],
      "metadata": {
        "id": "-iizBC3sotZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# A function that handles the input of the users.\n",
        "flag = True\n",
        "print(\"ChatBot: Hi! I will answer your queries. Please Ask. If you want to exit, type Bye!\")\n",
        "while flag:\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "    if user_response != 'bye':\n",
        "        if user_response == 'thanks' or user_response == 'thank you':\n",
        "            flag = False\n",
        "            print(\"ChatBot: You are welcome!\")\n",
        "        else:\n",
        "            if greeting(user_response) is not None:\n",
        "                print(\"ChatBot: \" + greeting(user_response))\n",
        "            else:\n",
        "                print(\"ChatBot: \" + response(user_response))\n",
        "    else:\n",
        "        flag = False\n",
        "        print(\"ChatBot: Chat with you later!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjPIi8-qou3g",
        "outputId": "5d2b500b-4ed5-414b-eeb0-cfda598890fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatBot: Hi! I will answer your queries. Please Ask. If you want to exit, type Bye!\n",
            "count oranges\n",
            "ChatBot: Oranges: There are a total of 7,200 oranges over 71 images\n",
            "giving on average 102 oranges per image.\n",
            "what is accuracy of labels?\n",
            "ChatBot: While a simple accuracy, or\n",
            "mean error, is appealing because it is easy to interpret, it is\n",
            "misleading because a model can have high accuracy over the\n",
            "entire data set, but low accuracy per image since the errors\n",
            "will wash each other out.\n",
            "bye\n",
            "ChatBot: Chat with you later!\n"
          ]
        }
      ]
    }
  ]
}